{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71778884-7463-43d6-b5a3-f9eb7f943cb7",
   "metadata": {},
   "source": [
    "# Handout 2\n",
    "EE-556: Mathematics of Data: From Theory to Computation - Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc3341-9ae5-4ae9-9c2d-7e03b7e3e8f1",
   "metadata": {},
   "source": [
    "In this handout, we will go through pen and paper exercises to get familiar with fundamental concepts that will be used throughout the course. We will take a look at convergence rates and smooth functions.\n",
    "\n",
    "<span style=\"font-variant:small-caps;\">Instructor: Prof. Volkan Cevher</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ecb66b-8e28-4908-91e6-a41bcf7988ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T08:05:13.930252Z",
     "start_time": "2024-07-22T08:05:13.920250Z"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abbe00b9-18e5-4b16-8ff0-8ebf2fcfc414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T08:05:14.178220Z",
     "start_time": "2024-07-22T08:05:14.169218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Queue(\n",
       "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "  [\"PreProcess\", MathJax.Hub],\n",
       "  [\"Reprocess\", MathJax.Hub]\n",
       ");\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Queue(\n",
    "  [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "  [\"PreProcess\", MathJax.Hub],\n",
    "  [\"Reprocess\", MathJax.Hub]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6c98b-6d08-40a5-b9c1-2401d9dc3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm schedulefree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800e4d73-eaf7-41fd-a6b6-dc890b3d92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch as pt\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam\n",
    "from Sophia import SophiaG\n",
    "from schedulefree import SGDScheduleFree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n = 10000\n",
    "p = 3\n",
    "sigma = 0.1\n",
    "\n",
    "def get_least_sq():\n",
    "    return nn.Linear(p, 1, bias=False)\n",
    "def get_small_nn(hidden_dim=100):\n",
    "    return nn.Sequential(nn.Linear(p, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea33e72-9c43-4fde-94b0-8dc2a32afb6c",
   "metadata": {},
   "source": [
    "## Interpreting convergence rates \n",
    "\n",
    "Throughout the course, we will often compare the performance of different methods by looking at convergence plots of different methods. It means that it is very important to be familiar with reading and drawing convergence rates. In the course, you will be mostly confronted with sublinear, linear and quadratic rates of convergence. Assume that you are given a sequence of iterates $(\\textbf{x}_k) \\in \\mathbb{R}^p$, converging towards a vector $\\textbf{x}^*$.\n",
    "\n",
    "- Then, the sequence $(\\textbf{x}_k)$ is said to converge **sublinearly** to $\\textbf{x}^*$ if $$\\lim_{k\\to\\infty} \\frac{\\|\\textbf{x}_{k+1}- \\textbf{x}^*\\|}{\\|\\textbf{x}_k- \\textbf{x}^*\\|} = 1$$ \n",
    "\n",
    "- The sequence $(\\textbf{x}_k)$ is said to converge **linearly** to $\\textbf{x}^*$ if, for some $c \\in (0,1)$, $$\\lim_{k\\to\\infty} \\frac{\\|\\textbf{x}_{k+1}- \\textbf{x}^*\\|}{\\|\\textbf{x}_k- \\textbf{x}^*\\|} = c$$ \n",
    "\n",
    "- The sequence $(\\textbf{x}_k)$ is said to converge **superlinearly** to $\\textbf{x}^*$ if, $$\\lim_{k\\to\\infty} \\frac{\\|\\textbf{x}_{k+1}- \\textbf{x}^*\\|}{\\|\\textbf{x}_k- \\textbf{x}^*\\|} = 0$$ \n",
    "\n",
    "    - The definition above can be refined by defining the *order of convergence*. The sequence $(\\textbf{x}_k)$ is said to converge **with order $q > 1$** to $\\textbf{x}^*$ if $$\\lim_{k\\to\\infty} \\frac{\\|\\textbf{x}_{k+1}- \\textbf{x}^*\\|}{\\|\\textbf{x}_k- \\textbf{x}^*\\|^q} < c$$ for some $c > 0$, not necessarily smaller than $1$. In particular, with $q=2$, we have **quadratic** convergence.\n",
    "\n",
    "### Problem 1: Convergence rate of different sequences.\n",
    "\n",
    "For each of the following sequences, find the limit $x^*$ and the the convergence rate.\n",
    "\n",
    "i) $x_k = \\displaystyle \\frac{1}{k+1}$ \n",
    "\n",
    "ii) $x_k =\\displaystyle\\frac{5k+\\log(k)}{3k+6}$\n",
    "\n",
    "iii) $x_k =\\displaystyle \\frac{3}{2}\\exp(-k/4)$\n",
    "\n",
    "iv) $x_k =\\displaystyle \\frac{1}{(3k)^2}$\n",
    "\n",
    "v) $x_k =\\displaystyle\\frac{1}{3^{2^k}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba10c75-7aae-4af6-ae34-b5c19b2ce07f",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283b37c-741e-458f-871a-fd22a083c164",
   "metadata": {},
   "source": [
    "### Problem 2: Drawing convergence rates\n",
    "\n",
    "Draw the asymptotic rate of convergence $\\|\\textbf{x}_k - \\textbf{x}^*\\|$ for the following sequences on either of the graphs that feature different scales (lin-lin, log-lin, log-log).\n",
    "\n",
    "i) $\\displaystyle \\frac{1}{k+1}$ \n",
    "\n",
    "ii) $\\displaystyle\\frac{1}{k^3+4}$\n",
    "\n",
    "iii) $\\displaystyle \\frac{3}{2}\\exp(-k/4)$\n",
    "\n",
    "iv) $\\displaystyle \\frac{1}{(3k)^2}$ \n",
    "\n",
    "\n",
    "**Note.** Each of the sequences can be naturally drawn on one of the scales. By natural, we mean that on some scale, the asymptotic behaviour of the sequence will be displayed as a line. For instance, a line in a log-log plot means that $\\log(\\|\\textbf{x}_k - \\textbf{x}^*\\|)$ is a *linear* function of $\\log(k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a50c77-4ddb-4e96-b35f-070d1fae0c33",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba62fd-f1ce-4541-acdf-1773443f592a",
   "metadata": {},
   "source": [
    "### Problem 3: Reading convergence plots\n",
    "On plots 1 and 2 below, the convergence rates of 5 methods are displayed (method $1$ is displayed on both plots). \n",
    "\n",
    "1. Characterize the rate of convergence (sublinear, linear, or quadratic) for each of the methods. Justify your answer.\n",
    "\n",
    "2. Establish more precisely the order of convergence of methods $1$, $3$, $4$ and $5$ by reading the plots.\n",
    "\n",
    "**Hint.** Find the slopes of the different lines and map, and use the scale of the plot to write the rate of convergence of the method.\n",
    "\n",
    "3. Rank methods $1$ to $5$ from the slowest to the fastest **asymptotic** rate of convergence, using the fact that method $1$ is displayed on both plots.\n",
    "\t\n",
    "|![](figs/plot1.png) | ![](figs/plot2.png)|\n",
    "|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf287d3-52d3-4bb3-a2f0-253435586056",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395bf09-e158-4ba6-ab92-cd24140d66b6",
   "metadata": {},
   "source": [
    "#### With real results\n",
    "Try running these two cells testing 4 popular optimization algorithms on a real least squares and binary classification problem, and from the displayed plots guage the convergence rates. Again, be careful on the scale of each axis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd9110-ac7b-4df8-97b5-327ffbc816b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "batch_size = n\n",
    "\n",
    "A = pt.randn(n, p)\n",
    "x = pt.randn(p)\n",
    "w = sigma * pt.randn(n)\n",
    "b = A @ x + w\n",
    "\n",
    "# Optimal objective known for least squares\n",
    "f_star = pt.dot(w, w).item() / n\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "models = [get_least_sq() for _ in range(4)]\n",
    "optims = [SGD(models[0].parameters(), lr=1e-2), \n",
    "          Adam(models[1].parameters(), lr=1e-2), \n",
    "          SophiaG(models[2].parameters(), lr=1e-2, rho=1e-4, weight_decay=0), \n",
    "          SGDScheduleFree(models[3].parameters(), lr=1e-2)]\n",
    "optims[3].train()\n",
    "losses = []\n",
    "\n",
    "for model, optim in tqdm(zip(models, optims), \"Optimizer\", 4):\n",
    "    losses_optim = []\n",
    "    for epoch in tqdm(range(num_epochs), \"Epoch\", leave=False):\n",
    "        for i in range(0, n, batch_size):\n",
    "            A_batch, b_batch = A[i:i+batch_size], b[i:i+batch_size]\n",
    "            optim.zero_grad()\n",
    "            loss = criterion(model(A_batch).squeeze(1), b_batch)\n",
    "            losses_optim.append(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    losses.append(np.array(losses_optim))\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "f.set_size_inches(2 * 6.4, 4.8)\n",
    "func_ = ax1.plot\n",
    "func2_ = ax2.semilogx\n",
    "for i, (losses_optim, style) in enumerate(zip(losses, [\"r-\", \"k-.\", \"b:\", \"c--\"])):\n",
    "    func_(np.arange(len(losses_optim)), losses_optim - f_star, style, label=f\"{i+1}\")\n",
    "    func2_(np.arange(len(losses_optim)), losses_optim - f_star, style, label=f\"{i+1}\")\n",
    "ax1.set_xlabel(\"$k$\")\n",
    "ax1.set_ylabel(\"$f(x_k)-f(x^{\\star})$\")\n",
    "ax2.set_xlabel(\"$k$\")\n",
    "ax2.set_ylabel(\"$f(x_k)-f(x^{\\star})$\")\n",
    "ax1.grid('both')\n",
    "ax2.grid('both')\n",
    "f.suptitle(\"Problem 1\")\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99214cb-86c8-4cc4-81c9-230283e0ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 250\n",
    "batch_size = 1024\n",
    "\n",
    "A = pt.randn(n, p)\n",
    "x = pt.randn(p)\n",
    "w = sigma * pt.randn(n)\n",
    "b = (A @ (x ** 2) + w < 1).long()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "models = [get_small_nn() for _ in range(4)]\n",
    "optims = [SGD(models[0].parameters(), lr=1e-2), \n",
    "          Adam(models[1].parameters(), lr=1e-2), \n",
    "          SophiaG(models[2].parameters(), lr=1e-2, rho=1e-4, weight_decay=0), \n",
    "          SGDScheduleFree(models[3].parameters(), lr=1e-2)]\n",
    "optims[3].train()\n",
    "losses = []\n",
    "\n",
    "for model, optim in tqdm(zip(models, optims), \"Optimizer\", 4):\n",
    "    losses_optim = []\n",
    "    for epoch in tqdm(range(num_epochs), \"Epoch\", leave=False):\n",
    "        for i in range(0, n, batch_size):\n",
    "            A_batch, b_batch = A[i:i+batch_size], b[i:i+batch_size]\n",
    "            optim.zero_grad()\n",
    "            loss = criterion(model(A_batch), b_batch)\n",
    "            losses_optim.append(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    losses.append(np.array(losses_optim))\n",
    "\n",
    "# Here we can only estimate the optimum\n",
    "f_star = min(map(min, losses))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "f.set_size_inches(2 * 6.4, 4.8)\n",
    "func_ = ax1.plot\n",
    "func2_ = ax2.loglog\n",
    "for i, (losses_optim, style) in enumerate(zip(losses, [\"r-\", \"k-.\", \"b:\", \"c--\"])):\n",
    "    func_(np.arange(len(losses_optim)), losses_optim - f_star, style, label=f\"{i+1}\")\n",
    "    func2_(np.arange(len(losses_optim)), losses_optim - f_star, style, label=f\"{i+1}\")\n",
    "ax1.set_xlabel(\"$k$\")\n",
    "ax1.set_ylabel(\"$f(x_k)-f(x^{\\star})$\")\n",
    "ax2.set_xlabel(\"$k$\")\n",
    "ax2.set_ylabel(\"$f(x_k)-f(x^{\\star})$\")\n",
    "ax1.grid('both')\n",
    "ax2.grid('both')\n",
    "f.suptitle(\"Problem 2\")\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23adba-2713-47af-a9b5-50211c104e02",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6883a5-3fd5-4bf2-ae65-2d8c47dade7a",
   "metadata": {},
   "source": [
    "### Problem 4: Convergence in accuracy against convergence in iterations\n",
    "\n",
    "Up to now, we have considered convergence in **iteration**, as a function of $k$. However, it is common to view the convergence as a function of the time require to reach a given accuracy $\\epsilon$. If we know that the $\\|\\textbf{x}_{k}- \\textbf{x}^*\\|\\leq \\frac{1}{k+1}$, the convergence in $\\epsilon$ tries to characterize the order of convergence as a function of the desired accuracy instead of the number of iterations. In practice, this amounts  to find $K(\\epsilon)$ such that $ \\forall k\\in \\mathbb{N}$, $k \\geq K(\\epsilon) \\Rightarrow \\|\\textbf{x}_{k+1}- \\textbf{x}^*\\| \\leq \\epsilon$.\n",
    "\n",
    "Given the convergence rate $\\|\\textbf{x}_{k}- \\textbf{x}^*\\|$ of a sequence, express the number of iterations required to reach a accuracy $\\epsilon$ for \n",
    "\n",
    "i) $\\displaystyle \\frac{1}{k+1}$\n",
    "\n",
    "ii) $\\displaystyle\\frac{1}{k^3+4}$\n",
    "\n",
    "iii) $\\displaystyle \\frac{3}{2}\\exp(-k/4)$ \n",
    "\n",
    "iv) $\\displaystyle \\frac{1}{(3k)^2}$ \n",
    "\n",
    "v) $\\displaystyle\\frac{1}{3^{-2^k}}$\n",
    "\n",
    "vi) $\\displaystyle\\frac{4}{\\sqrt{k+3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f040bff-4da7-4b03-a9ab-8b7db77b8140",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d208cd-48c0-4a13-9a83-d8f7e7ec3a47",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Smooth functions\n",
    "Throughout the course, we will frequently encounter $L$-smooth functions.\n",
    "**Definition.** A function $f:\\mathcal{Q}\\to\\mathbb{R}$ is said to be $L$-smooth with respect to a pair of dual norms ($\\|\\cdot\\|$, $\\|\\cdot\\|_{ * }$) if there exists some $L>0$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\|\\nabla f(\\textbf{x})-\\nabla f(\\textbf{y}) \\|_* \\leq L \\|\\textbf{x} -\\textbf{y}\\| \\text{     } \\forall \\textbf{x}, \\textbf{y} \\in \\mathcal{Q}.\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "The Lipschitz constant of the the gradient $L$, also called the smoothness constant, can be computed in several ways, and we will explore different ways to obtain it in the following exercises.\n",
    "\n",
    "### Problem 5: Lipschitz gradient in the one-dimensional case\n",
    "In a single dimensional case, we have a function $f:\\mathcal{Q}\\subseteq \\mathbb{R} \\to \\mathbb{R}$. The equation (1) can be restated as\n",
    "$$|f'(x)-f'(y)| \\leq L|x-y| \\text{     } \\forall \\textbf{x}, \\textbf{y} \\in \\mathcal{Q}.$$\n",
    "Prove that the smoothness constant $L$ can be computed as the maximum of the absolute value of the second derivative, i.e. $L=\\max_{z\\in\\mathcal{Q}} |f''(z)|$.\n",
    "\n",
    "<span style=\"font-variant:small-caps;\">Hint.</span> Use the mean value theorem.\n",
    "\n",
    "<span style=\"font-variant:small-caps;\">Remark.</span> This statement can be extended to higher dimensional cases, but one needs to be careful to appropriately define the norms that will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23e2ab6-fbac-46a2-ba93-645d8af2c68f",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cbd4d-34df-443d-ab3e-568d7f2068f2",
   "metadata": {},
   "source": [
    "### Problem 6: Lipschitz gradient in the quadratic case\n",
    "We now move to a multidimensional case, where we have $f:\\mathbb{R}^p \\to \\mathbb{R}$ defined as $f(\\textbf{x})=\\frac{1}{2} \\textbf{x}^\\top A\\textbf{x}$, where $A$ is a positive semi-definite matrix. We will explore a different way to compute the Lipschitz constant of the gradient in this setting.\n",
    "\n",
    "Given a pair of dual norms ($\\|\\cdot\\|_p,\\|\\cdot\\|_q$) with $\\frac{1}{p} + \\frac{1}{q}=1$,  prove that when $$\\|\\nabla f(\\textbf{x})-\\nabla f(\\textbf{y}) \\|_q \\leq L \\|\\textbf{x} -\\textbf{y}\\|_p \\text{     } \\forall \\textbf{x}, \\textbf{y} \\in \\mathcal{Q},$$ then $L=\\|A\\|_{p\\to q}$.\n",
    "\n",
    "<span style=\"font-variant:small-caps;\">Hint.</span> Recall the definition of the operator norm from the lecture. $$\\|A\\|_{p\\to q}:= \\sup_{\\textbf{x}:\\|\\textbf{x}\\|_p \\leq 1} \\|A\\textbf{x}\\|_q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821b866-9e6e-4957-9281-b341c4b37ddb",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a677-9455-4a30-b70e-0e8e90189278",
   "metadata": {},
   "source": [
    "### Problem 7: Operator norms in action\n",
    "\n",
    "1. Given  $A\\in\\mathbb{R}^{m\\times n}$ and $a_i^\\top$ the $i$-th row of $A$, prove that the operator norm $\\|\\textbf{A}\\|_{1\\to \\infty} = \\max_{i\\in\\{1,\\dots, m\\}}\\|a_i \\|_\\infty$.\n",
    "\n",
    "2. Consider the matrix $$A = \\begin{bmatrix} 2 & \\frac{1}{\\sqrt{2}} & -1 \\\\ \\frac{1}{\\sqrt{2}} & 3 & -\\frac{1}{\\sqrt{2}} \\\\ -1& -\\frac{1}{\\sqrt{2}} &2\\end{bmatrix}.$$ Compute the Lipschitz constant of the gradient of $f(\\textbf{x}) = \\textbf{x}^\\top A \\textbf{x}$ in the following settings.\n",
    "\n",
    "|$\\|\\nabla f(\\textbf{x})-\\nabla f(\\textbf{y}) \\|_\\infty \\leq L \\|\\textbf{x} -\\textbf{y}\\|_1$ | $\\|\\nabla f(\\textbf{x})-\\nabla f(\\textbf{y}) \\|_2 \\leq L \\|\\textbf{x} -\\textbf{y}\\|_2$|\n",
    "|-|-|\n",
    "\n",
    "Are the values of $L$ equal? How do you interpret the result?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480228ed-7f6d-4262-b619-be00c2b08741",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39d212-6e28-4913-8afe-12cdb16e7d7e",
   "metadata": {},
   "source": [
    "#### Try verifying the result with code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322b6c6-cfab-4d52-8d05-9bce14ef49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 1/np.sqrt(2), -1], [1/np.sqrt(2), 3, -1/np.sqrt(2)], [-1, -1/np.sqrt(2), 2]])\n",
    "\n",
    "op_norm_1_inf = ???\n",
    "\n",
    "op_norm_2_2 = ???\n",
    "\n",
    "op_norm_1_inf.round(4), op_norm_2_2.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236e286-7e85-440f-8583-55a78067a108",
   "metadata": {},
   "source": [
    "### Problem 8: The importance of choosing the smoothness norm\n",
    "\n",
    "1. During the lectures we saw that the L-smoothness of a function $f$ gives rise to local quadratic upper-bounds. The iterative minimization of these upper bounds recovers the well-known Gradient Descent (GD) method. As a warm-up, let us  remind ourselves of the computation.\n",
    "\n",
    "Let $f: \\mathbb{R}^d \\to \\mathbb{R}$ be convex and $L_2$-smooth and recall from the lecture that this implies\n",
    "\\begin{equation}\n",
    "\\tag{2}\n",
    "f(y) \\leq f(x) + \\langle\\nabla f(x), y - x \\rangle + \\frac{L_2}{2} \\|x - y\\|_2^2, \\quad \\forall x, y \\in  \\mathbb{R}^d.\n",
    "\\end{equation}\n",
    "Show that the minimizer in $y$ of the right-hand side of (2) is \n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{3}\n",
    "y^* = x - \\frac{1}{L_2} \\nabla f(x). \n",
    "\\end{equation}\n",
    "\n",
    "Observe that setting $x = x_k$ and letting $x_{k+1} := y^*$ in (3) results precisely in the update rule of GD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274a2b1-080b-42df-9b49-33993e4614e4",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e1ad1-5080-4251-b282-63fe746faa60",
   "metadata": {},
   "source": [
    "2. In point 1. we arrive at the GD update rule by considering the smoothness of $f$ with respect to the Euclidean norm. However, smoothness may be considered with respect to arbitrary norms $\\|\\cdot \\|_p$, and its general expression is given by \n",
    "\n",
    "\\begin{equation}\n",
    "    \\|\\nabla f(x) -\\nabla f(y)\\|_q \\leq L_p \\|x - y\\|_p,\n",
    "    \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\|z\\|_q := \\max_{\\|t\\|_p \\leq 1} \\langle z, t\\rangle $ is the dual norm of $\\|\\cdot\\|_p$. As in the case of smoothness with respect to $\\|\\cdot\\|_2$, smoothness with respect to $\\|\\cdot \\|_p$ induces a local quadratic upper bound as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "     f(y) \\leq f(x) + \\langle\\nabla f(x), y - x \\rangle + \\frac{L_p}{2} \\|x - y\\|_p^2, \\quad \\forall x, y \\in  \\mathbb{R}^d.\n",
    "     \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "By iteratively minimizing the right-hand side of (5) and depending on the chosen $p$, one arrives at various non-Euclidean gradient methods. The choice of norm is important as it can result in asymptotically faster gradient methods than the traditional GD. An example can be found in the work of [3], who leveraged smoothness with respect to $\\|\\cdot\\|_{\\infty}$ to obtain superior convergence for the maximum s-t flow and maximum concurrent multicommodity flow problems.\n",
    "\n",
    "In the following, we will guide you in discovering the update rule that emerges from considering smoothness in the $\\ell_{\\infty}$-norm. Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be convex with $L_{\\infty}$-Lipschitz gradient $\\|\\nabla f(x) -\\nabla f(y)\\|_1 \\leq L_{\\infty} \\|x - y\\|_{\\infty}$.\n",
    "\n",
    "  (a) Define\n",
    "    \n",
    "\\begin{equation} \n",
    "        [x]^{\\#} := \\underset{{s \\in \\mathbb{R}^d}}{\\operatorname{arg\\,max}} \\left\\{ \\langle x, s\\rangle - \\frac{1}{2} \\|s\\|^2_{\\infty} \\right\\}.\n",
    "\t\t\\tag{6}\n",
    "\\end{equation}\n",
    "    \n",
    "Show that $\\; \\|x\\|_1 \\,\\mathrm{sgn}(x) \\in [x]^{\\#} \\; $, i.e. that it is a maximizer of the expression in (6).\n",
    "    \n",
    "***Hint:*** You can use H&ouml;lder's inequality below to find an upper bound, then show that it is correspondingly attained.\n",
    "\n",
    "\\begin{equation*}\n",
    "        |\\langle x, y\\rangle| \\leq \\|x\\|_p \\|y\\|_q \\qquad \\forall p, q \\in [1, \\infty] \\text{ s.t. } \\frac{1}{p} +  \\frac{1}{q} = 1 \\text{ (with the convention that } \\frac{1}{\\infty} = 0 ).\n",
    "    \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bece4c7-6dc5-4bdc-8836-d003bf99b49c",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e968fb-7c54-4929-a6a5-ea71038e1b68",
   "metadata": {},
   "source": [
    "(b) Using inequality (5) adapted to the $\\|\\cdot\\|_{\\infty}$ norm, show that the minimizer in $y$ of its right-hand side is given by \n",
    "    \\begin{equation*}\n",
    "        y^* = x - \\frac{1}{L_{\\infty}} \\|\\nabla f(x)\\|_1\\mathrm{sgn}\\left(\\nabla f(x)\\right).\n",
    "    \\end{equation*}\n",
    "    \n",
    "Similar to point 1., observe how letting $x = x_k$ and $x_{k+1} := y^*$ gives us an update rule. This type of update pertains to the so-called SignGD method.\n",
    "\n",
    "***Hint:*** Write down the relevant $\\underset{}{\\operatorname{arg\\,max}}$ expression and then try to transform it equivalently such that the $\\underset{}{\\operatorname{arg\\,max}}$ formulation from (6) appears. \n",
    "\n",
    "***Remark:*** For those interested in doing further reading on the topic, [2] and [1] are good places to start. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102311a0-4cdf-4ec9-9c19-d71422c94271",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f73dc-2bfa-4fd1-b8b5-e96a8f55ad72",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signsgd: Compressed optimisation for non-convex problems.\n",
    "arXiv preprint arXiv:1802.04434, 2018.\n",
    "\n",
    "[2] D. E. Carlson, E. Collins, Y.-P. Hsieh, L. Carin, and V. Cevher. Preconditioned spectral descent for deep learning. In Advances in\n",
    "Neural Information Processing Systems, pages 2971–2979, 2015.\n",
    "\n",
    "[3] J. A. Kelner, Y. T. Lee, L. Orecchia, and A. Sidford. An almost-linear-time algorithm for approximate max flow in undirected graphs,\n",
    "and its multicommodity generalizations. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pages\n",
    "217–226. SIAM, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
