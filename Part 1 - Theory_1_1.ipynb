{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "#### EE-556 Mathematics of Data - Fall 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we consider a binary classification task that we will model using logistic regression. Your goal will be to find a classifier using first-order methods and accelerated gradient descent methods. The first part will consist of more theoretical questions, and the second one will ask you to implement these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  ℹ️ <strong>Information on group based work:</strong>\n",
    "</div>\n",
    "\n",
    "- You are to deliver only 1 notebook per group.\n",
    "- Asking assistance beyond your group is ok, but answers should be individual to the group.\n",
    "- In the event that there was <span style=\"color: red;\">disproportional work done</span> by different group members, let the TAs know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #f00; background-color: #fdd; padding: 10px; border-radius: 5px;\">\n",
    "  ⚠️ Do not forget: Write who are the people in your group as well as their respective SCIPER numbers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person 1 **Name**: || Person 1 **SCIPER**:\n",
    "\n",
    "\n",
    "Person 2 **Name**: || Person 2 **SCIPER**:\n",
    "\n",
    "\n",
    "Person 3 **Name**: || Person 3 **SCIPER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression - 15 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a classic approach to _binary classification_. Before we dive in, let us first define the standard logistic function $\\sigma$ on which most of what follows is built:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sigma : x \\mapsto \\frac{1}{1 + \\exp{(-x)}}.\n",
    "\\end{equation*}\n",
    "\n",
    "In logistic regression, we model the _conditional probability_ of observing a class label $b$ given a set of features $\\mathbf{a}$. More formally, if we observe $n$ independent samples\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{a}_i\\in\\mathbb{R}^p$ and $b_i\\in\\{-1, +1\\}$ is the class label, we _assume_ that $b_i$ given $\\mathbf{a}_i$ is a symmetric Bernouilli random variable with parameter $\\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)$, for some unknown $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$. In other words, we assume that there exists an $\\mathbf{x}^\\natural \\in \\mathbb{R}^p$ such that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = 1 \\mid \\mathbf{a}_i) = \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural) \\quad \\text{ and } \\quad \\mathbb{P}(b_i = -1 \\mid \\mathbf{a}_i) = 1 - \\sigma(\\mathbf{a}_i^T\\mathbf{x}^\\natural)=  \\sigma( - \\mathbf{a}_i^T\\mathbf{x}^\\natural).\n",
    "\\end{equation*}\n",
    "\n",
    "This is our statistical model. It can be written in a more compact form as follows,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma(j \\cdot \\mathbf{a}_i^T\\mathbf{x}^\\natural), \\quad j \\in \\{+1, -1\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal now is to determine the unknown $\\mathbf{x}^\\natural$ by constructing an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ (1 point) We are provided with a set of $n$ independent observations. Show that the negative log-likelihood $f$ can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tf(\\mathbf{x}) = -\\log(\\mathbb{P}(b_1, \\dots, b_n | a_1, \\dots, a_n)) & = \\sum_{i=1}^n  \\log(1 + \\exp(- b_i \\mathbf{a}_i^T\\mathbf{x})).\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "By definition of loglikelihood:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log \\mathbb{P}(b_1, \\dots, b_n \\mid \\mathbf{a}_1, \\dots, \\mathbf{a}_n; \\mathbf{x}) = \\sum_{i=1}^n \\log \\sigma(b_i \\mathbf{a}_i^T \\mathbf{x}).\n",
    "\\end{equation*}\n",
    "\n",
    "Using the definition of $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $, we can express $ \\log \\sigma(x) $ as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log \\sigma(x) = -\\log(1 + e^{-x}).\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore, the negative log-likelihood becomes:\n",
    "\n",
    "\\begin{equation*}\n",
    "f(\\mathbf{x}) = -\\sum_{i=1}^n \\log \\sigma(b_i \\mathbf{a}_i^T \\mathbf{x}) = \\sum_{i=1}^n \\log(1 + e^{-b_i \\mathbf{a}_i^T \\mathbf{x}}).\n",
    "\\end{equation*}\n",
    "q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ (2 point) Show that the function $u \\mapsto \\log(1 + \\exp(-u))$ is convex. Deduce that $f$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "It is enough to show that $g(x)=\\log(1+\\exp(-x))$ is twice-diff. and it's second derivative is non-negative for all ral values x.\n",
    "\n",
    "By the chain rule we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "g'(x)=\\frac{1}{1+\\exp(-x)}\\cdot(-\\exp(-x))=-\\frac{1}{\\exp(x)+1}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "g''(x)=\\frac{\\exp(x)}{(\\exp(x)+1)^2} > 0,\n",
    "\\end{equation*}\n",
    "$\\forall x\\in \\mathbb{R}$. Thus, the second derivative is positive and $g(x)$ is convex. Obv. the sum of convex functions is convex q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator, which is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}^\\star_{ML} = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}),\n",
    "\\end{equation*}\n",
    "\n",
    "is a global minimum so it can serve as an estimator of $\\mathbf{x}^\\natural$. But, does the minimum always exist? We will ponder this question in the following three points.\n",
    "\n",
    "__(c)__ (1 point) Explain the difference between infima and minima.  Give an example of a convex function, defined over $\\mathbb{R}$, that does not attain its infimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "In general, minimum is an infimum which can be achieved, i.e. stays in the observed set, whereas infimum may not be in the set.\n",
    "\n",
    "Consider $x \\to \\exp(x)$. Clearly, it is convex and defined over all real values. \n",
    "\n",
    "Still, it's infimum is $0$. For any (small) $\\varepsilon > 0:$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\varepsilon = exp(x_1), \\quad \\text{so} \\quad x_1=\\log(\\varepsilon),\n",
    "\\end{equation*}\n",
    "\n",
    "but\n",
    "0  doesn't equal $exp(x_2)$, for any real $x_2.$ q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (1 point) Let us assume that there exists $\\mathbf{x}_0 \\in \\mathbb{R}^p$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad b_i \\mathbf{a}_i^T\\mathbf{x}_0 > 0.\n",
    "\\end{equation*}\n",
    "\n",
    "This is called _complete separation_ in the literature. Can you think of a geometric reason why this name is appropriate? Think of a 2D example where this can happen (i.e $p=2$) and describe why _complete separation_ is an appropriate name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Geometrically, the $\\mathbf{x}_0$ separates 0s and 1s as a hyperspace in the whole space $\\mathbb{R}^{p}$.\n",
    "\n",
    "This hyperspace is given by the set of $n$ equations $\\{\\mathbf{a}_i^{T}\\mathbf{x}_0=0\\}_{i=1}^n$ (which, by the algebra theorems, defines a hyperspace as the points $\\mathbf{x}_0$ which satisfy the equations). \n",
    "\n",
    "In 2D setting, we may have the line which separates the points of two types (like, red's and green's completely). This way we have the indeed complete separation. q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you should see that it is likely that our data satisfies the complete separation assumption. Unfortunately, as you will show in the following question, this can become an obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(e)__ (1 point) In a _complete separation_ setting, i.e, there exists $\\mathbf{x}_0$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\forall i\\in\\{1, \\dots, n\\}, \\quad \\quad b_i \\mathbf{a}_i^T\\mathbf{x}_0 > 0,\n",
    "\\end{equation*}\n",
    "\n",
    "prove that the function $f$ does not attain its minimum. \n",
    "\n",
    "__Hint__: If the function did have a minimum, would it be above, below or equal to zero? Then think of how $f(2 \\mathbf{x}_0)$ compares with $f(\\mathbf{x}_0)$, how about $f(\\alpha \\mathbf{x}_0)$ for $\\alpha \\rightarrow + \\infty$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "It is easy to see that for $\\alpha\\to+\\infty$:\n",
    "\n",
    "\\begin{equation*}\n",
    "-b_i \\mathbf{a}_i^T(\\alpha\\mathbf{x}_0) = \\alpha\\cdot(- b_i \\mathbf{a}_i^T\\mathbf{x}_0) \\to -\\infty.\n",
    "\\end{equation*}\n",
    "\n",
    "This means that \n",
    "\\begin{equation*}\n",
    "\\exp(-b_i \\mathbf{a}_i^T(\\alpha\\mathbf{x}_0))\\to 0+,\\text{ as }\\alpha\\to+\\infty.\n",
    "\\end{equation*}\n",
    "So,\n",
    "\\begin{equation*}\n",
    "\\lim_{\\alpha\\to+\\infty}\\log(1 + \\exp(- b_i \\mathbf{a}_i^T(\\alpha\\mathbf{x}_0))=\n",
    "\\lim_{\\alpha\\to+\\infty}\\log(1+0+)=0+.\n",
    "\\end{equation*}\n",
    "Here $0+$ means the right convergence to the corresponding value. Thus, the infimum of $f(x)$ is 0 and it is not achieveable, as $f$ is always non-negative. q.e.d. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have just shown convex functions do not always attain their infimum. So it is possible for the maximum-likelihood estimator $\\mathbf{x}^\\star_{ML}$ to not exist. We will resolve this issue by adding a regularizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we consider the function\n",
    "\n",
    "\\begin{equation*}\n",
    "\tf_\\mu(\\mathbf{x}) = f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2\n",
    "\\end{equation*}\n",
    "with $\\mu> 0$.\n",
    "\n",
    "__(f)__ (1 point) Show that the gradient of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla f_\\mu(\\mathbf{x}) = \\sum_{i=1}^n -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "__Hint__: Lecture 3 shows you how to proceed with this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Compute the derivative of one term\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{d}{dx} \\log(1 + \\exp(-b_i \\mathbf{a}_i^T \\mathbf{x})) = \\frac{1}{1 + \\exp(-b_i \\mathbf{a}_i^T \\mathbf{x})} \\cdot \\frac{d}{dx} \\left(1 + \\exp(-b_i \\mathbf{a}_i^T \\mathbf{x})\\right).\n",
    "\\end{equation*}\n",
    "\n",
    "Now, \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{d}{dx} \\exp(-b_i \\mathbf{a}_i^T \\mathbf{x}) = -b_i \\exp(-b_i \\mathbf{a}_i^T \\mathbf{x}) \\cdot \\mathbf{a}_i.\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, the gradient of $ f_i(\\mathbf{x}) $ becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla f_i(\\mathbf{x}) = \\frac{-b_i \\exp(-b_i \\mathbf{a}_i^T \\mathbf{x}) \\mathbf{a}_i}{1 + \\exp(-b_i \\mathbf{a}_i^T \\mathbf{x})}\n",
    "= -b_i\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i\n",
    "\\end{equation*}\n",
    "\n",
    "We also clearly have that \n",
    "\\begin{equation*}\n",
    "\\frac{d}{dx} \\frac{\\mu}{2}||\\mathbf{x}||_2^2 = \\frac{d}{dx} \\frac{\\mu}{2}\\sum_{i=1}^p (x_i^2)  = \\frac{\\mu}{2}\\cdot2\\cdot \\mathbf{x}=\\mu\\mathbf{x}.\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore \n",
    "\\begin{equation*}\n",
    "\\nabla \\frac{\\mu}{2}||\\mathbf{x}||_2^2 = \\mu\\mathbf{x}.\n",
    "\\end{equation*}\n",
    "\n",
    "The resulting gradient of the whole $f_{\\mu}(x)$ is just the whole sum of gradients of the terms and the regularizer. q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(g)__ (1 point) Show that the Hessian of $f_\\mu$ can be expressed as \n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) = \\sum_{i=1}^{n} \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x}))\\mathbf{a}_i\\mathbf{a}_i^T + \\mu \\mathbf{I}.\n",
    "\\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "The Hessian of $f_{\\mu}$ is the $\\nabla$ of the $\\nabla f_{\\mu}(x)$.\n",
    "\n",
    "Again, computing for each term we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla^2 f_i(\\mathbf{x}) \n",
    "=\\nabla (-b_i\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i)\n",
    "=-b_i\\nabla(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x}))\\mathbf{a}_i.\n",
    "\\end{equation*}\n",
    "\n",
    "As we know, $\\nabla\\sigma(\\mathbf{x})=\\sigma(\\mathbf{x})(1-\\sigma(\\mathbf{x})).$ Thus, by the chain rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla\\sigma(-b_i\\mathbf{a}_i^T\\mathbf{x})=\n",
    "\\sigma(-b_i\\mathbf{a}_i^T\\mathbf{x})(1-\\sigma(-b_i\\mathbf{a}_i^T\\mathbf{x}))\\cdot (-b_i\\cdot\\mathbf{a}_i^T).\n",
    "\\end{equation*}\n",
    "\n",
    "Since $b_i=\\pm 1$, $b_i^2=1$. So\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla^2 f_i(\\mathbf{x}) = \n",
    "\\sigma(-b_i\\mathbf{a}_i^T\\mathbf{x})(1-\\sigma(-b_i\\mathbf{a}_i^T\\mathbf{x}))\\mathbf{a}_i\\mathbf{a}_i^T.\n",
    "\\end{equation*}\n",
    "\n",
    "Clearly, derivative w.r. to $\\mathbf{x}$ of $\\mu\\mathbf{x}$ is just $\\mu$, which transforms to $\\mu\\mathbf{I}$ in the matrix case.\n",
    "\n",
    "Now, the Hessian of $f_\\mu(x)$ is just the sum of $\\nabla^2 f_i(x)$ and hessian of regularizer. q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to observe that we can write the Hessian in a more compact form by defining the matrix\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{a}_1^T & \\rightarrow \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_2^T & \\rightarrow \\\\\n",
    "         &  \\ldots &  \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_n^T & \\rightarrow \\\\\n",
    "  \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "It is easy to see that we have\n",
    "\\begin{equation}\n",
    "\t\\nabla^2 f_\\mu(\\mathbf{x}) =  \\mathbf{A}^T \\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) \\right)\\mathbf{A}+ \\mu \\mathbf{I}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(h)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We may use the $\\mu$-strong convexity def. in terms of Hessian of $f_\\mu(x)$, whose minimum eigenvalue must be at least $\\mu$.\n",
    "\n",
    "Following the **(g)** we have that the matrix $\\mathbf{A}^T \\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) \\right)\\mathbf{A}$ should have eigenvalues at least 0.\n",
    "\n",
    "Obviously, $\\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) \\right)$ does not affect the positiveness of the Hessian (values of $\\sigma$ are positive and less than $1$ anyways). \n",
    "\n",
    "So in the end we need to know if $A^TA$ has non-negative eigenvalues, which is true:\n",
    "\n",
    "$$\n",
    "x^TA^TAx= (Ax)^TAx = ||Ax||^2_2 \\geq 0,\n",
    "$$\n",
    "for every non-zero vector $x$. Thus, this matrix is positive semi-definite, or have non-negative eigenvalues, and the proof is complete. q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(i)__ (3 points) Is it possible for a strongly convex function to not attain its minimum ? <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Justify your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We know that for a $\\mu$-strongly convex func. $f(x)$ there exists convex $h(x)$ (with the same domain), such that\n",
    "$$\n",
    "h(x) = f(x)-\\frac{\\mu}{2}||x||_2^2.\n",
    "$$\n",
    "Let's fix $\\mathbf{x}_0$ from the domain of $f$. As $h$ is convex, then it has nonempty subgradient set $\\partial h$ for each $\\mathbf{x}_0$, say $w\\in\\partial h(\\mathbf{x}_0)$,\n",
    "such that\n",
    "$$\n",
    "w\\cdot (\\mathbf{x}-\\mathbf{x}_0)\\leq h(\\mathbf{x})-h(\\mathbf{x}_0),\n",
    "$$ where $\\cdot$ means dot-product of two vectors. Thus\n",
    "$$\n",
    "w\\cdot (\\mathbf{x}-\\mathbf{x}_0)+\\frac{\\mu}{2}||x||_2^2\\leq f(\\mathbf{x})-h(\\mathbf{x}_0).\n",
    "$$\n",
    "From the last inequality we have that $f(\\mathbf{x})\\to+\\infty$ as $x\\to\\infty$ (because of the $||x||_2^2$, which obv. goes to $+\\infty$ faster than the dot-product potentially can).\n",
    "\n",
    "This means that the level-set \n",
    "$$\n",
    "A(\\mathbf{x}_0)=\\{x|f(x)\\leq f(x_0) \\}\n",
    "$$ is bounded (in other case, we select sequence such that $f(x)\\to+\\infty$).\n",
    "\n",
    "Now, if \n",
    "$$\n",
    "\\{\\mathbf{x}_k\\}\\subset A(\\mathbf{x}_0):\\lim_{k\\to\\infty}\\mathbf{x}_k=\\mathbf{x},\n",
    "$$\n",
    "then we have\n",
    "$$\n",
    "f(\\mathbf{x})\\leq\\liminf_{k\\to\\infty}f(\\mathbf{x}_k)\\leq f(\\mathbf{x}_0),\n",
    "$$\n",
    "where the first $\\leq$ is true because $f$ is convex (thus, lower-semicontinuous), and the second $\\leq$ is true because each $f(\\mathbf{x}_k)\\leq f(\\mathbf{x}_0)$ (being from the level-set). \n",
    "\n",
    "This means that $A$ is closed and bounded, so compact. Thus, by continuity $f$ attains it's minimum on $A$, and, by the def. of $A$, minimizer on A is also a global minimizer. q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show that $f_\\mu$ is smooth, i.e, $\\nabla f_\\mu$ is L-Lipschitz with respect to the Euclidean norm, with \n",
    "\\begin{equation}\n",
    "\tL = \\|A\\|^2_F + \\mu \\text{, where }\\|\\cdot\\|_F\\text{ denotes the Frobenius norm. }\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point for all three questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We know that matrix $rank(\\mathbf{a}_i\\mathbf{a}_i^T)=1$, as it is an outer product of two vectors (so the columns are all prop. to a_i), thus it has only one non-zero eigenvalue. Now\n",
    "$$\n",
    "\\mathbf{a}_i\\mathbf{a}_i^T\\mathbf{a}_i = \\mathbf{a}_i\\cdot(\\mathbf{a}_i^T\\mathbf{a}_i) = ||\\mathbf{a}_i||_2^2 \\mathbf{a}_i,\n",
    "$$\n",
    "thus $||\\mathbf{a}_i||_2^2$ is indeed an eigenvalue. As it is unique non-zero, it is then $\\lambda_{\\max}$. q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-2)__ Using [2](#mjx-eqn-eq2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$. \n",
    "\n",
    "__Hint__: Recall that $\\lambda_{\\max}(\\cdot)$ verifies the triangle inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We use the triangle inequality for $\\lambda_\\max$. So\n",
    "$$\n",
    "\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{x})) \\leq \\lambda_\\max(\\mathbf{A}^T \\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) \\right)\\mathbf{A}) + \\lambda_\\max(\\mu \\mathbf{I}).\n",
    "$$\n",
    "We obv. have \n",
    "$$\n",
    "\\lambda_\\max(\\mu \\mathbf{I}) = \\mu,\n",
    "$$\n",
    "And each value in $\\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) \\right)$ can't be more than 1 and less than 0, so \n",
    "$$\n",
    "\\lambda_\\max(\\mathbf{A}^T \\text{Diag}\\left(\\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})(1 - \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})) \\right)\\mathbf{A})\\leq \\lambda_\\max(\\mathbf{A}^T\\mathbf{A}).\n",
    "$$\n",
    "Now, for the Frobenius norm of $\\mathbf{A}$ we have \n",
    "$$\n",
    "\\sigma_{max}^2\\leq\\sum_{i=1}^n\\sigma_i^2 = ||\\mathbf{A}||^2_F = \\sum_{i=1}^n||\\mathbf{a}_i||_2^2,\n",
    "$$\n",
    "where $\\sigma_i$ are the singular values of $\\mathbf{A}$, and $\\sigma_{max}^2=\\lambda_{\\max}(\\mathbf{A}^T\\mathbf{A}).$ q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(j-3)__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|A\\|_F^2 + \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We just see that the largest eigenvalue of the Hessian is upper-bounded by $\\|\\mathbf{A}\\|_F^2 + \\mu$, and that is exavtly the def. of L-smoothness for the desired L. q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(l)__ (2 point) To finalize, we introduce the Kullback-Leibler (KL) divergence. The KL divergence is a measure of how one probability distribution differs from a second, reference probability distribution. Along side the examples we saw in slide 18 of Lecture 1, the KL divergence is also a useful loss function to be used in learning frameworks.\n",
    "\n",
    "Write the definition of the Kullback-Leibler (KL) divergence between the true label distribution $q(b_i)$ and the model’s predicted distribution $p(b_i∣\\mathbf{a}_i)$ and show that minimizing the KL divergence between $q(b_i)$ and $p(b_i∣\\mathbf{a}_i)$ is equivalent to minimizing the negative log-likelihood derived in (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "We have the def. of DKL:\n",
    "$$\n",
    "\\mathbf{D}_{KL}(q(b)||p(b|\\mathbf{a})) = \\mathbb{E}_{b\\sim q(b|\\mathbf{a})}\\log\\left(\\frac{q(b|\\mathbf{a})}{p(b|\\mathbf{a})}\\right)\n",
    "=\\mathbb{E}_{b\\sim q(b|\\mathbf{a})}(\\log(q(b|\\mathbf{a})))-\\mathbb{E}_{b\\sim q(b|\\mathbf{a})}(\\log(p(b|\\mathbf{a}))),\n",
    "$$\n",
    "and as the first term does not matter within the argmin problem, we have\n",
    "$$\n",
    "\\textbf{argmin}_{b}\\mathbf{D}_{KL}(q(b)||p(b|\\mathbf{a})) = \\textbf{argmin}_{b}(-\\mathbb{E}_{b\\sim q(b|\\mathbf{a})}(\\log(p(b|\\mathbf{a})))_,\n",
    "$$\n",
    "and then obv. we can write\n",
    "$$\n",
    "-\\mathbb{E}_{b\\sim q(b|\\mathbf{a})}(\\log(p(b|\\mathbf{a})))=-\\frac{1}{n}\\sum_{i=1}^n \\log p(b_i|\\mathbf{a}_i),\n",
    "$$\n",
    "which completes the proof. q.e.d.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From your work in this section, you have shown that the maximum likelihood estimator for logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_2^2$ regularizer. Consequently, the estimator for $\\mathbf{x}^\\natural$ we will use will be the solution of the smooth strongly convex problem,\n",
    "\\begin{equation}\n",
    "\t\\mathbf{x}^\\star=\\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^p} f(\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|_2^2.\n",
    "\\tag{3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) TA's will give you candy if you provide a complete proof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
