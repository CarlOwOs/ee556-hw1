{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVcEtGJ8QeYP"
   },
   "source": [
    "# 3. Sharpness-aware minimization (SAM) wirh Sparse Networks - 25 points\n",
    "\n",
    "## 3.1 Get a sparse networks through pruning\n",
    "**Pruning** is a technique used to reduce the size and complexity of a neural network model by removing (setting to zero) less important parameters. The goal is to create a more efficient model that retains its predictive accuracy while being smaller, which can improve both inference speed and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilWbesreILh4"
   },
   "source": [
    "Let's train a simple model on the MNIST dataset to learn about pruning at first. We just use 10% of the dataset for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJta4OIBcEpy"
   },
   "source": [
    "### 3.1.1 Train a dense network with SGD\n",
    "Let us first train a dense model with SGD. We reuse the model for the discriminator of the GAN in Homework 2 and name it 'Classifier'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXSZdsCILh4"
   },
   "outputs": [],
   "source": [
    "from lib.part3.utils import *\n",
    "max_epochs = 10\n",
    "device = \"cpu\" # Change this if you can and want to use a GPU device\n",
    "model = Classifier().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLI-Alb-ILh5"
   },
   "source": [
    "We define the optimizing process of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oXZG0iPPILh5"
   },
   "outputs": [],
   "source": [
    "def optimize_sgd(model, optimizer, img, label):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(img)\n",
    "    loss = cross_entropy(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udzDwcQqX-GE"
   },
   "source": [
    "The following cell runs the training loop, this might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y_p6FbG0ILh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.932 accuracy on the validation set.\n",
      "Epoch 1 with 0.952 accuracy on the validation set.\n",
      "Epoch 2 with 0.957 accuracy on the validation set.\n",
      "Epoch 3 with 0.957 accuracy on the validation set.\n",
      "Epoch 4 with 0.961 accuracy on the validation set.\n",
      "Epoch 5 with 0.961 accuracy on the validation set.\n",
      "Epoch 6 with 0.962 accuracy on the validation set.\n",
      "Epoch 7 with 0.966 accuracy on the validation set.\n",
      "Epoch 8 with 0.967 accuracy on the validation set.\n",
      "Epoch 9 with 0.967 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, optimize_sgd, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnvFT-DxILh6"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LKTkbxGyILh6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9771 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgVCA_jwJbPx"
   },
   "source": [
    "### 3.1.2 Sparse network with magnitude-based pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubynCgs2b0wT"
   },
   "source": [
    "Magnitude-based pruning specifically focuses on **removing weights that have the smallest absolute values**, under the assumption that weights with smaller magnitudes contribute less to the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkjQPvAXcZcI"
   },
   "source": [
    "**(1)** (6 points) Realize magnitude-based pruning below, which removes a part of weights that have the smallest absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xfb19GShJZFI"
   },
   "outputs": [],
   "source": [
    "def magnitude_prune(model, prune_fraction):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            # FILL: Get weight's absolute values\n",
    "            weight_abs = torch.abs(param.data)\n",
    "\n",
    "            # FILL: Compute the threshold\n",
    "            threshold = torch.quantile(weight_abs, prune_fraction)\n",
    "            \n",
    "            # FILL: Prune weights below the threshold\n",
    "            mask = weight_abs >= threshold\n",
    "            with torch.no_grad():\n",
    "                param.data = param.data * mask\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fr0VzGC_NPtv"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# Copy a model for pruning\n",
    "sparse_model = copy.deepcopy(model)\n",
    "# Get a sparse model by pruning 50% parameters\n",
    "sparse_model = magnitude_prune(sparse_model, prune_fraction=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sOOCdBoRgzL"
   },
   "source": [
    "Copy a sparse model for SAM implementation later in 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xzi_XqlmRbC4"
   },
   "outputs": [],
   "source": [
    "sparse_model_sam = copy.deepcopy(sparse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_EFs4FiQh1X"
   },
   "source": [
    "Evaluation after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4ZtXqwuaPtNF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.8461 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz5gPbMuGiAW"
   },
   "source": [
    "### 3.1.3 Finetune the sparse model\n",
    "\n",
    "We finetune the sparse model after pruning with SGD to recover its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jD0tZKllNi00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.887 accuracy on the validation set.\n",
      "Epoch 1 with 0.887 accuracy on the validation set.\n",
      "Epoch 2 with 0.887 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "finetune_epoch = 3\n",
    "train_model(sparse_model, optimizer, optimize_sgd, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rha2SQXBQnV8"
   },
   "source": [
    "Evaluate the sparse model after finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sby9YSR9PrYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.8796 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHZY8_nZaMO-"
   },
   "source": [
    "**(2)** (2 point) What are the pros and cons of sparse networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "\n",
    "Pros: \n",
    "* Sparse networks require less storage because we can use sparse matrix representations.\n",
    "* Inference is faster by skipping operations with zero weights.\n",
    "* Sparsification can be a regularizer and reduce overfitting.\n",
    "\n",
    "Cons:\n",
    "* Loss of model performance.\n",
    "* Difficulty in fine-tuning.\n",
    "* Implementation complexity/limitations: taking advantage of the sparsity in the parameters can require libraries, datastructure, and hardware support. Moreover, hardware accelerators optimized for dense matrix operations may not benefit of the sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97IxkV9Rl5bT"
   },
   "source": [
    "## 3.2 Train the sparse model with SAM\n",
    "\n",
    "Sharpness-aware minimization (SAM) is a new optimization technique, which is satisfied with not just a low loss, instead it seeks a neighborhood with uniformly low loss. SAM is motivated by the link between the geometry of the loss landscape and generalization. It makes sense that a low loss within a uniformly low loss neighborhood will generalize better than a low loss within a region of higher variance.\n",
    "\n",
    "To be specific, we consider a model with the weight vector of $\\mathbf{w}$ and the training loss $L_S$. SAM aims to minimize the maximum loss within a small region which is usually a $\\ell_2$ ball with $\\rho$ radius. Note that $\\rho$ is a small value close to $0$. Therefore, SAM can be formulated as a minimax optimization problem:\n",
    "$$\\min_{\\mathbf{w}} \\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2\\leq \\rho} L_S (\\mathbf{w} + \\mathbf{\\epsilon})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFY_FOCEILh3"
   },
   "source": [
    "**(3)** (3 points) Please solve the inner maximum problem by first-order Taylor expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u8vP9FCtTuh"
   },
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "For small perturbations $ \\mathbf{\\epsilon} $ , we can approximate the loss function $ L_S(\\mathbf{w} + \\mathbf{\\epsilon}) $ using a first-order Taylor expansion around $ \\mathbf{w} $\n",
    "\n",
    "$$\n",
    "L_S (\\mathbf{w} + \\mathbf{\\epsilon}) \\approx L_S (\\mathbf{w}) + \\nabla L_S (\\mathbf{w})^\\top \\mathbf{\\epsilon}.\n",
    "$$\n",
    "This gives us the problem\n",
    "$$\n",
    "\\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2 \\leq \\rho} \\left[ L_S (\\mathbf{w}) + \\nabla L_S (\\mathbf{w})^\\top \\mathbf{\\epsilon} \\right]\n",
    "= L_S (\\mathbf{w}) + \n",
    "\\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2 \\leq \\rho} \\nabla L_S (\\mathbf{w})^\\top \\mathbf{\\epsilon},\n",
    "$$\n",
    "since $ L_S (\\mathbf{w}) $ is constant with respect to $ \\mathbf{\\epsilon} $.\n",
    "\n",
    "The above is a linear maximization problem over an $ \\ell_2 $-ball of radius $ \\rho $ . As we know, the solution to this problem is given by aligning $ \\mathbf{\\epsilon} $ with the gradient $ \\nabla L_S (\\mathbf{w}) $ :\n",
    "\n",
    "$$\n",
    "\\mathbf{\\epsilon}^* = \\rho \\cdot \\frac{\\nabla L_S (\\mathbf{w})}{\\|\\nabla L_S (\\mathbf{w})\\|_2}\n",
    "$$\n",
    "\n",
    "Thus in the end we have:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2 \\leq \\rho} L_S (\\mathbf{w} + \\mathbf{\\epsilon}) \\approx L_S (\\mathbf{w}) + \\rho \\|\\nabla L_S (\\mathbf{w})\\|_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwbpa28BILh6"
   },
   "source": [
    "**(4)** (8 points) Now we will train the same model using the SAM optimizer.\n",
    "Please implement SAM by the two steps below. The first step is for the maximizer which calculates $\\epsilon$ obtained in question (1). The second step is the normal step for the minimizer: $\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta_t \\nabla L_S (\\mathbf{w}_t + \\mathbf{\\epsilon}_t)$ where $\\eta_t$ is step size. Note that we set $\\rho=0.05$.\n",
    "\n",
    "Hint: be careful about weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7mZeWh05ILh6"
   },
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, lr=0.01, rho=0.05):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, lr)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        # Note that p.grad gets the gradient; p.data gets the weight.\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        norm += 1e-12 # Avoid zero norm\n",
    "        return norm\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self):\n",
    "        # Add the perturbation on the weight.\n",
    "        grad_norm = self._grad_norm()  # Compute the norm of the gradient\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = self.defaults[\"rho\"] * p.grad / grad_norm\n",
    "                self.state[p]['e_w'] = e_w\n",
    "                p.add_(e_w)\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        self.base_optimizer.step()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = self.state[p]['e_w']\n",
    "                p.sub_(e_w)\n",
    "                p.sub_(self.defaults[\"lr\"] * p.grad)\n",
    "\n",
    "        self.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F6EaFSWILh6"
   },
   "source": [
    "Define an optimizer of `SAM` for the model. We recommend using `SGD` as base optimizer with a learning rate of $0.05$ (which is same with SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f3D9IZGql74k"
   },
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.SGD\n",
    "sam_optimizer = SAM(sparse_model_sam.parameters(), base_optimizer, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwYNhqGoILh6"
   },
   "source": [
    "**(5)** (4 points) Please define the optimizing process of SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q9-zRaSVCwGO"
   },
   "outputs": [],
   "source": [
    "def optimize_sam(model, optimizer, img, label):\n",
    "\n",
    "    enable_running_stats(model)\n",
    "    # First forward-backward pass\n",
    "    # Hint: use sam_optimizer above\n",
    "    # Hint: use loss 'cross_entropy'\n",
    "\n",
    "    output = model(img)\n",
    "    loss = torch.nn.functional.cross_entropy(output, label) \n",
    "    loss.backward()\n",
    "    optimizer.first_step()\n",
    "\n",
    "    disable_running_stats(model)\n",
    "    \n",
    "    # Second forward-backward pass\n",
    "    \n",
    "    output = model(img)\n",
    "    loss = torch.nn.functional.cross_entropy(output, label)\n",
    "    loss.backward()    \n",
    "    optimizer.second_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "45KsL4m4DAU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.951 accuracy on the validation set.\n",
      "Epoch 1 with 0.956 accuracy on the validation set.\n",
      "Epoch 2 with 0.962 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(sparse_model_sam, sam_optimizer, optimize_sam, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhSh_GOxoNE-"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the sparse model finetuned with SAM on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3IAfT8aMoNWb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9765 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model_sam)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi2NLImh_QeD"
   },
   "source": [
    "**(6)** (2 points) Give a conclusion comparing SAM with SGD. Is there any drawback of SAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "SAM tends to avoid sharp minima, which are linked to poor generalization. This comes at the cost of increased computation, as SAM requires two forward-backward passes per optimization step. \n",
    "\n",
    "The radius hyperparameter of SAM may potentially affect its performance."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
