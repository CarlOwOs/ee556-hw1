





%%javascript
MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
});


%%javascript
MathJax.Hub.Queue(
  ["resetEquationNumbers", MathJax.InputJax.TeX],
  ["PreProcess", MathJax.Hub],
  ["Reprocess", MathJax.Hub]
);


!pip install tqdm schedulefree


import numpy as np
from tqdm.notebook import tqdm
import torch as pt
from torch import nn
from torch.optim import SGD, Adam
from Sophia import SophiaG
from schedulefree import SGDScheduleFree
from matplotlib import pyplot as plt

n = 10000
p = 3
sigma = 0.1

def get_least_sq():
    return nn.Linear(p, 1, bias=False)
def get_small_nn(hidden_dim=100):
    return nn.Sequential(nn.Linear(p, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 2))























num_epochs = 1000
batch_size = n

A = pt.randn(n, p)
x = pt.randn(p)
w = sigma * pt.randn(n)
b = A @ x + w

# Optimal objective known for least squares
f_star = pt.dot(w, w).item() / n

criterion = nn.MSELoss()
models = [get_least_sq() for _ in range(4)]
optims = [SGD(models[0].parameters(), lr=1e-2), 
          Adam(models[1].parameters(), lr=1e-2), 
          SophiaG(models[2].parameters(), lr=1e-2, rho=1e-4, weight_decay=0), 
          SGDScheduleFree(models[3].parameters(), lr=1e-2)]
optims[3].train()
losses = []

for model, optim in tqdm(zip(models, optims), "Optimizer", 4):
    losses_optim = []
    for epoch in tqdm(range(num_epochs), "Epoch", leave=False):
        for i in range(0, n, batch_size):
            A_batch, b_batch = A[i:i+batch_size], b[i:i+batch_size]
            optim.zero_grad()
            loss = criterion(model(A_batch).squeeze(1), b_batch)
            losses_optim.append(loss.item())
            loss.backward()
            optim.step()
    losses.append(np.array(losses_optim))


f, (ax1, ax2) = plt.subplots(1, 2)
f.set_size_inches(2 * 6.4, 4.8)
func_ = ax1.plot
func2_ = ax2.semilogx
for i, (losses_optim, style) in enumerate(zip(losses, ["r-", "k-.", "b:", "c--"])):
    func_(np.arange(len(losses_optim)), losses_optim - f_star, style, label=f"{i+1}")
    func2_(np.arange(len(losses_optim)), losses_optim - f_star, style, label=f"{i+1}")
ax1.set_xlabel("$k$")
ax1.set_ylabel("$f(x_k)-f(x^{\star})$")
ax2.set_xlabel("$k$")
ax2.set_ylabel("$f(x_k)-f(x^{\star})$")
ax1.grid('both')
ax2.grid('both')
f.suptitle("Problem 1")
ax1.legend()
ax2.legend()
plt.show()


num_epochs = 250
batch_size = 1024

A = pt.randn(n, p)
x = pt.randn(p)
w = sigma * pt.randn(n)
b = (A @ (x ** 2) + w < 1).long()


criterion = nn.CrossEntropyLoss()
models = [get_small_nn() for _ in range(4)]
optims = [SGD(models[0].parameters(), lr=1e-2), 
          Adam(models[1].parameters(), lr=1e-2), 
          SophiaG(models[2].parameters(), lr=1e-2, rho=1e-4, weight_decay=0), 
          SGDScheduleFree(models[3].parameters(), lr=1e-2)]
optims[3].train()
losses = []

for model, optim in tqdm(zip(models, optims), "Optimizer", 4):
    losses_optim = []
    for epoch in tqdm(range(num_epochs), "Epoch", leave=False):
        for i in range(0, n, batch_size):
            A_batch, b_batch = A[i:i+batch_size], b[i:i+batch_size]
            optim.zero_grad()
            loss = criterion(model(A_batch), b_batch)
            losses_optim.append(loss.item())
            loss.backward()
            optim.step()
    losses.append(np.array(losses_optim))

# Here we can only estimate the optimum
f_star = min(map(min, losses))

f, (ax1, ax2) = plt.subplots(1, 2)
f.set_size_inches(2 * 6.4, 4.8)
func_ = ax1.plot
func2_ = ax2.loglog
for i, (losses_optim, style) in enumerate(zip(losses, ["r-", "k-.", "b:", "c--"])):
    func_(np.arange(len(losses_optim)), losses_optim - f_star, style, label=f"{i+1}")
    func2_(np.arange(len(losses_optim)), losses_optim - f_star, style, label=f"{i+1}")
ax1.set_xlabel("$k$")
ax1.set_ylabel("$f(x_k)-f(x^{\star})$")
ax2.set_xlabel("$k$")
ax2.set_ylabel("$f(x_k)-f(x^{\star})$")
ax1.grid('both')
ax2.grid('both')
f.suptitle("Problem 2")
ax1.legend()
ax2.legend()
plt.show()
































A = np.array([[2, 1/np.sqrt(2), -1], [1/np.sqrt(2), 3, -1/np.sqrt(2)], [-1, -1/np.sqrt(2), 2]])

op_norm_1_inf = ???

op_norm_2_2 = ???

op_norm_1_inf.round(4), op_norm_2_2.round(4)





















